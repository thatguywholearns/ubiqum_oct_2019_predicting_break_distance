---
title: "Cars report - David Verbiest"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=FALSE}
#Import all used libraries
library(readr)
library(ggplot2)

#Import dataset_cars and rename the variables
dataset_cars <- read.csv('./R Tutorial Data Sets/cars.csv', header = TRUE)
names(dataset_cars) <- c("name_of_car", "speed_of_car", "distance_of_car")
```

# Executive summary

The goal of this analysis is to **predict the breaking distance of car** based on its **speed before breaking**. We used a **linear model** to predict the breaking distance of the car. To get the best performing linear model we **transformed the speed of the car**. We **squared** the speed of the car, as this relationship between the squared speed and the breaking distance are linear. We estimated the **following model**

$$y = 3.17 + 0.15x$$

In this equation x stands for the squared speed. This means that for every increase in squared speed of the car the breaking distance increase by 0.15. The **model perfromed well** in terms of predicting test data, data never seen by the model before as estimated model **explained 99% of the variance** in the dependent variable. 

This can also be seen graphically as the **graphical representation** of the model **closely fits the test datapoints**.





# Analysis

## Exploration of the data

### The Data Set

Below you see the first *5 lins* of the used data set.

```{r, echo=FALSE}
knitr::kable(dataset_cars[1:5,], align = "c")
```


### Summary Data Set

```{r, echo=FALSE}
summary_cars <- summary(dataset_cars)
knitr::kable(summary_cars, align = "c")
```


### Boxplot: *Distance Of Car*

```{r fig.align="center", echo=FALSE}
ggplot(dataset_cars, aes(y = distance_of_car)) + geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4)
```

```{r, echo=FALSE, align="c"}
quantile <- quantile(dataset_cars$distance_of_car)
knitr::kable(quantile, align="c")
```

We can see that the there is **one outlier**. The following data point:

```{r fig.align="center", echo=FALSE}
outliers <- boxplot(dataset_cars$distance_of_car, plot=FALSE)$out
outliers_instance <- dataset_cars[which(dataset_cars$distance_of_car %in% outliers), ]
knitr::kable(outliers_instance, align="c")
```


## Preprocessing 

In order not to bias the prediction we remove this data point from the training data.

```{r}
dataset_cars <- dataset_cars[-which(dataset_cars$distance_of_car %in% outliers),]
```


We create a scatter plot between the variables speed of car and distance of car in order to spot any type of relationship between the two variables.

```{r fig.align="center", echo=FALSE}
plot(dataset_cars$distance_of_car, dataset_cars$speed_of_car)
```

We see that there is pattern present, however the pattern is not linear. Distance a car needs to break increases for each marginal speed increase, hinting towards a quadratic relationship. This also makes sense given that velocity and acceleration (or in this case decelartion, which is can be seen as negative acceleration) are described by a second degree polynomials. 

In order to fit a line through the data that is able to predict the data we need to transform the distance variable. We square the variable so that the both variables increase at the same rate. 

```{r fig.align="center"}
dataset_cars$speed_of_car_squared <- dataset_cars$speed_of_car*dataset_cars$speed_of_car
```

When we plot the the variable speed of car squared against the distance of car variable we see that relationship is linear.

```{r fig.align="center", echo=FALSE}
plot(dataset_cars$distance_of_car, dataset_cars$speed_of_car_squared)
```


## Modeling

In order to estimate our model we use 70% of instances randomly selected from the data. We use the remaining 30% to test the resulting model. (We use the random seed:123)

```{r, echo=FALSE}

set.seed(123)

#Create training set and test set by using a 70/30 split 
train_size <- round(nrow(dataset_cars)*0.70)
test_size <- nrow(dataset_cars) - train_size
training_indices <- sample(seq_len(nrow(dataset_cars)), size = train_size)
training_set <- dataset_cars[training_indices,]
test_set <- dataset_cars[-training_indices,]

#Fit Linear model
lm_model1 <- lm(distance_of_car~ speed_of_car_squared, training_set)
summary_model1 <- summary(lm_model1)
print(summary_model1)

#Store coefficients and equation of final model
lm_model1_coef <- coefficients(lm_model1)
lm_model1_intercept <- lm_model1_coef[1]
lm_model1_coef1 <- intercept <- lm_model1_coef[2]
lm_model1_eq <- paste("estimated model: y = ", round(lm_model1_intercept, 2), " + ", round(lm_model1_coef1, 2), " *x" )
```

We can see that the p-value for our estimated parameters is less than 0.05 meaning that the estimates for our parameters significant with a confidence level 95%. The model also has a R squared of 98.9% on the training, meaning that our model explains 98.9% of the variation in the data.

## Testing

We test our model on subset of test data that the model hasn't seen before. The results are the following.

```{r fig.align="center", echo=FALSE}
#Make predictions on test set
lm_model1_test <- lm(distance_of_car~ speed_of_car_squared, test_set)
output_lm_model1_test <- summary(lm_model1_test)
print(output_lm_model1_test)
predictions_model1 <- predict(lm_model1, test_set)

#Add predictions and prediction error to dataframe test data and plot predictions against real values
test_set$prediction <- predictions_model1
test_set$error <- test_set$distance_of_car - predictions_model1
test_set$relative_error <- paste(round(((test_set$error / test_set$distance_of_car)*100), digits = 2), "%")
ggplot(test_set, aes(x = speed_of_car_squared, y = distance_of_car)) + geom_point() + geom_abline(intercept = lm_model1_intercept, slope = lm_model1_coef1) + ggtitle(lm_model1_eq)
```

When we run our model on the test data we see that the R squared increases to 99.18%
In other words our model can make strong predictions of the distance a car needs to break based on the speed of that car. We can also see this graphically as we see the the model closely alligns with the test data.


```{r, echo=FALSE}
#ggplot(test_set, aes(y = error))
```
When we take a coser look at our errors we can see that the most errors are around 0, indicating   that we have good model and we extracted all information from the data






